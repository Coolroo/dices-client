{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"well\" style=\"margin:1em 2em\">\n",
    "<p>This Notebook reproduces and expands on a demo from ‚ÄúDistant Reading of Direct Speech in Epic: An Illustrated Workflow,‚Äù a talk I gave at the FIEC / CA annual meeting in London, July 8, 2019.</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "# Heroes and their moms\n",
    "\n",
    "Let's say we're young scholars interested in Telemachus' speech to Penelope.\n",
    " - How often does he speak to her?\n",
    " - What kind of language does he use?\n",
    " - How does the narrator refer to these speeches?\n",
    " \n",
    "We'll start by showing how the DICES database and Python library can be used to retrieve and manipulate the speeches in question. Then we'll expand our perspective to show how DICES enables research on a \"distant reading\" scale, taking in all heroes and their mothers. Finally, we'll check the accuracy of the automated methods by comparing against a benchmark of hand-curated mother-child speech data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this lets me change the api while the notebook is open\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DICES API\n",
    "\n",
    "See example 1 for notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dicesapi import DicesAPI\n",
    "api = DicesAPI(\n",
    "    dices_api = 'http://localhost:8000/api',\n",
    "    cts_api = 'http://cts.perseids.org/api/cts/',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tokenize.word import WordTokenizer\n",
    "tokenizer = {\n",
    "    'greek': WordTokenizer('greek'),\n",
    "    'latin': WordTokenizer('latin'),\n",
    "}\n",
    "\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "from cltk.lemmatize.greek.backoff import BackoffGreekLemmatizer\n",
    "lemmatizer = {\n",
    "    'greek': BackoffGreekLemmatizer(),\n",
    "    'latin': BackoffLatinLemmatizer(),    \n",
    "}\n",
    "\n",
    "# regular expressions to tidy up perseus texts for ctlk\n",
    "replacements = {\n",
    "    'greek': [\n",
    "        (r'¬∑', ','),           # FIXME: raised dot? \n",
    "        (chr(700), chr(8217)), # two different apostrophes that look alike\n",
    "    ],\n",
    "    'latin': [\n",
    "        \n",
    "    ],\n",
    "}\n",
    "\n",
    "# compile the regexes\n",
    "for lang in ['greek', 'latin']:\n",
    "    replacements[lang] = [(re.compile(pat), repl) for pat, repl in replacements[lang]]\n",
    "    \n",
    "\n",
    "# generic tokenize-lemmatize function\n",
    "def lemmatize(text, lang):\n",
    "    '''return a set of (token,lemmata) pairs for a string'''\n",
    "    \n",
    "    for pat, repl in replacements[lang]:\n",
    "        text = pat.sub(repl, text)\n",
    "    \n",
    "    tokens = tokenizer[lang].tokenize(text)\n",
    "    lemmata = lemmatizer[lang].lemmatize(tokens)\n",
    "    \n",
    "    return lemmata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WikiData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwikidata.linked_data_interface import get_entity_dict_from_api\n",
    "from qwikidata.entity import WikidataItem, WikidataProperty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 1\n",
    "\n",
    "Let's start by building a lexicon for all the words Telemachus speaks to Penelope.\n",
    "\n",
    "### Identify and download the speeches\n",
    "\n",
    "Using the hand-rolled DICES API code, we can search speeches using keywords. For now, JSON results from the API are paged, so if your search has a lot of results, you may have to wait for several pages to download. I've added a progress bar widget because I get impatient.\n",
    "\n",
    "Note that I can specify both the speaker and the addressee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = api.getSpeeches(spkr_name='Telemachus', addr_name='Penelope')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in speeches:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the passages from a remote library\n",
    "\n",
    "We have the metadata for each speech; now we need the text. The DICES library uses MyCapytain under the hood to retrieve the passages from a remote CTS server: here, Perseus, specified in the `DicesAPI()` call above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = []\n",
    "for s in speeches:\n",
    "    cts_passage = s.getCTS()\n",
    "    text = cts_passage.text\n",
    "    passages.append(text)\n",
    "    \n",
    "    print(f'{s.author.name} {s.work.title} {s.l_range}')\n",
    "    print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CLTK to parse the text\n",
    "\n",
    "We can use CTLK's tokenizers to break each string into meaningful units -- sentences and/or words. Then we use the backoff lemmatizer to normalize all the inflected forms to dictionary headwords.\n",
    "\n",
    "I rolled these steps into one convenience function up above. üëâüèª *One thing to watch out for is that CTLK needs to know what language you're working with, so I've added a kludge to set language based on the author name; really, that should be built into DICES eventually.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lems = Counter()\n",
    "for p in passages:\n",
    "    lang = s.getLang()\n",
    "    lemmatized = lemmatize(p.lower(), lang)\n",
    "    \n",
    "    these_lems = [lem for tok, lem in lemmatized]\n",
    "    lems.update(these_lems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the counter to a Pandas data frame for tidier presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(lems.most_common(), columns=['lemma', 'count'])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Now let's think more broadly. How typical is this kind of speech? We can use external linked data to find other examples of mother-son conversations in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some custom code to query WikiData\n",
    "\n",
    "This lets us ask whether a given addressee belongs to the set of people having a certain relationship to a given speaker. It takes a while to download the WikiData entities, and I had to run this a number of times, so I cached WD data in the respective character objects once it's downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkWD(c):\n",
    "    '''make sure character has wikidata id'''\n",
    "    if c.char is not None:\n",
    "        if c.char.wd is not None:\n",
    "            if len(c.char.wd.strip()) > 0:\n",
    "                return c.char.wd.strip()\n",
    "\n",
    "def checkWDRelation(s, a, relation, cache=None):\n",
    "    if cache is None:\n",
    "        cache = {}\n",
    "    else:\n",
    "        if (s.id, a.id) in cache:\n",
    "            return cache[(s.id, a.id)]\n",
    "\n",
    "    res = False\n",
    "\n",
    "    if not hasattr(s, 'wd_ent'):\n",
    "        s.wd_ent = WikidataItem(get_entity_dict_from_api(s.wd))\n",
    "\n",
    "    claim_group = s.wd_ent.get_truthy_claim_group(relation)\n",
    "\n",
    "    for claim in claim_group:\n",
    "        if claim.mainsnak.datavalue is None:\n",
    "            continue\n",
    "        if claim.mainsnak.datavalue.value['id'] == a.wd:\n",
    "            res = True\n",
    "    \n",
    "    cache[(s.id, a.id)] = res\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the relation \"mother of\" has the WikiData ID `'P25'`. Here's how we ask if a given addressee is the mother of a given speaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker = api.getCharacters(name='Telemachus')[0]\n",
    "addressee = api.getCharacters(name='Penelope')[0]\n",
    "\n",
    "print(f'Is {addressee.name} the mother of {speaker.name}?')\n",
    "print(checkWDRelation(speaker, addressee, 'P25'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also added a separate cache just for the boolean result of checkWDRelation, to save a little more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_mothers = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using WikiData to filter the speeches\n",
    "\n",
    "The DICES dataset includes WikiData ids for most of the characters (not all). The DICES API doesn't let us query WikiData itself, though. For now, the easiest thing for now is just to download all the speeches and character IDs, and then cross reference them against WikiData using its own API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all the speeches: takes a minute\n",
    "speeches = api.getSpeeches(progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check each speaker-addressee pair against WikiData**\n",
    "\n",
    "What we actually do here is download the WikiData entity for each speaker, if we don't already have it cached. Then we ask the WD entity for it's mom(s), and check the WD ID of the addressee against the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "\n",
    "# create a progress bar\n",
    "pbar = widgets.IntProgress(\n",
    "    value = 0,\n",
    "    min = 0,\n",
    "    max = len(speeches),\n",
    "    bar_style='info',\n",
    "    orientation='horizontal'\n",
    ")\n",
    "pbar_label = widgets.Label(value = f'{pbar.value}/{len(speeches)}')\n",
    "display(widgets.HBox([pbar, pbar_label]))\n",
    "\n",
    "for s in speeches:\n",
    "    if s.spkr is not None and s.addr is not None:\n",
    "        for spkr in s.spkr:\n",
    "            spkr_wd = checkWD(spkr)\n",
    "            if spkr_wd is not None:\n",
    "\n",
    "                for addr in s.addr:\n",
    "                    addr_wd = checkWD(addr)\n",
    "                    if addr_wd is not None:\n",
    "                        df.append((\n",
    "                            s.id,\n",
    "                            s.work.title,\n",
    "                            s.l_fi,\n",
    "                            s.l_la,\n",
    "                            spkr.char.name, spkr_wd, \n",
    "                            addr.char.name, addr_wd,\n",
    "                            checkWDRelation(spkr.char, addr.char, 'P25', cache=cache_mothers),\n",
    "                            checkWDRelation(addr.char, spkr.char, 'P25', cache=cache_mothers)\n",
    "                            ))\n",
    "    pbar.value += 1\n",
    "    pbar_label.value = f'{pbar.value}/{len(speeches)}'\n",
    "\n",
    "df = pd.DataFrame(df, columns=['id', 'work', 'l_first', 'l_last', 'spkr', 'sp_wd', 'addr', 'ad_wd', 'sp_is_mom', 'ad_is_mom'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î Let's take a look at the results. Here is the complete set of speeches, with the additional attribute `sp_is_mom` if the speaker is the addressee's mother, and `ad_is_mom` if the addressee is the speaker's mother.\n",
    "\n",
    "As a quick sanity check, the first two speeches in the Argonautica, which were at the top of the list when I ran this, are between Jason and his mother, Alcimede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to pandas, we can filter the data frame on the new boolean columns to show only speeches between mother and child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = df.loc[df['sp_is_mom'] | df['ad_is_mom'],\n",
    "             ['work', 'l_first', 'l_last', 'spkr', 'addr']]\n",
    "hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas also comes in handy if I wanted to export this data to Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('example.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "Let's see how well the automated approach worked. We'll load up a hand-corrected list of mother-child speeches and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = pd.read_csv('data/moms-bench.csv', dtype=str)\n",
    "bench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the union of `hits` and `bench` to see how we did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = hits.merge(bench, on=['work', 'l_first'], how='outer', \n",
    "                        suffixes=['_h', '_b'], indicator=True)\n",
    "results[['work', 'l_first', 'spkr_h', 'addr_h', 'spkr_b', 'addr_b', '_merge']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos = sum(results['_merge'] == 'both')\n",
    "\n",
    "p = true_pos / hits.shape[0]\n",
    "r = true_pos / bench.shape[0]\n",
    "\n",
    "print(f'Precision: {p:.2f}')\n",
    "print(f'Recall:    {r:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "#### The good news\n",
    "Well, we returned no false positives, and managed to catch 80% of the benchmark set.\n",
    "\n",
    "#### The bad news\n",
    "Lets' look a little more closely at the speeches we missed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed = results[results['_merge'] == 'right_only'][\n",
    "                ['work', 'l_first', 'spkr_h', 'addr_h', 'spkr_b', 'addr_b', 'notes']]\n",
    "missed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a glance, I'd say these fall into two groups:\n",
    "\n",
    " 1. A conversation in the Iliad between Hera and a group of gods, some of whom were here children\n",
    " 2. Conversations in the Aeneid between Venus and Aeneas\n",
    " \n",
    "Missing the first group seems somewhat understandable, if the addressee wasn't explicitly named. Missing the second group, on the other hand, is a big problem!\n",
    "\n",
    "#### Digging a little deeper\n",
    "\n",
    "First, let's confirm that all these speeches are in the database results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed.merge(df, how='left', on=['work', 'l_first'])[[\n",
    "    'work', 'l_first',                               # keys: work and locus\n",
    "    'id', 'spkr', 'addr', 'sp_is_mom', 'ad_is_mom',  # cols from df\n",
    "    'spkr_b', 'addr_b'                               # cols from bench\n",
    "    \n",
    "]].groupby('id').agg(lambda x: ', '.join(x.unique())) # for speeches with multiple speakers/\n",
    "                                                      # addressees: combine on speech id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, all the missed speeches were in the database. The ones involving Hera's children as a collective 'gods' clearly would be difficult to match with this method.\n",
    "\n",
    "But what happened with Aeneas and Venus?? WikiData must not treat Venus as Aeneas' mother...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker = api.getCharacters(name='Aeneas')[0]\n",
    "addressee = api.getCharacters(name='Venus')[0]\n",
    "\n",
    "print(f'Is {addressee.name} the mother of {speaker.name}?')\n",
    "print(checkWDRelation(speaker, addressee, 'P25'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after some head scratching, the problem turns out to be that WikiData has separate entries for [Venus](https://www.wikidata.org/wiki/Q47652) and [Aphrodite](https://www.wikidata.org/wiki/Q35500). Only the latter is listed as a mother of [Aeneas](https://www.wikidata.org/wiki/Q82732)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker = api.getCharacters(name='Aeneas')[0]\n",
    "addressee = api.getCharacters(name='Aphrodite')[0]\n",
    "\n",
    "print(f'Is {addressee.name} the mother of {speaker.name}?')\n",
    "print(checkWDRelation(speaker, addressee, 'P25'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "\n",
    " - Part of the fault here is ours: the database underlying DICES betrays its diverse origins by a lingering heterogeneity. If we think Aphrodite and Venus are the same character, we'd better make sure we refer to her the same way consistently, or risk weird results.\n",
    " \n",
    " - WikiData gave us a lot for free -- all of the individual mother-child relationships were in there when we knew where to look.\n",
    " \n",
    " - But missing Venus was a pretty big \"gotcha\" in the end.\n",
    " \n",
    " - If we want to rely on linked open data for high-stakes work, we need resources that are sensitive to the details we care about. We hope that MANTO, because it's specific to Classical myth and hand-curated by domain experts, can help us with problems like when to treat Venus and Aphrodite as independent entities and when to consider them identical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
