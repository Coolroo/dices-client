{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"well\" style=\"margin:1em 2em\">\n",
    "<p>This Notebook reproduces and expands on a demo from “Distant Reading of Direct Speech in Epic: An Illustrated Workflow,” a talk I gave at the FIEC / CA annual meeting in London, July 8, 2019.</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "# Heroes and their moms\n",
    "\n",
    "Let's say we're young scholars interested in Telemachus' speech to Penelope.\n",
    " - How often does he speak to her?\n",
    " - What kind of language does he use?\n",
    " - How does the narrator refer to these speeches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this lets me change the api while the notebook is open\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DICES API\n",
    "\n",
    "See example 1 for notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dicesapi import DicesAPI\n",
    "api = DicesAPI(\n",
    "    dices_api = 'https://fierce-ravine-99183.herokuapp.com/api',\n",
    "    cts_api = 'http://cts.perseids.org/api/cts/',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tokenize.word import WordTokenizer\n",
    "tokenizer = {\n",
    "    'greek': WordTokenizer('greek'),\n",
    "    'latin': WordTokenizer('latin'),\n",
    "}\n",
    "\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "from cltk.lemmatize.greek.backoff import BackoffGreekLemmatizer\n",
    "lemmatizer = {\n",
    "    'greek': BackoffGreekLemmatizer(),\n",
    "    'latin': BackoffLatinLemmatizer(),    \n",
    "}\n",
    "\n",
    "# regular expressions to tidy up perseus texts for ctlk\n",
    "replacements = {\n",
    "    'greek': [\n",
    "        (r\"·\", ','),           # FIXME: raised dot? \n",
    "        (chr(700), chr(8217)), # two different apostrophes that look alike\n",
    "    ],\n",
    "    'latin': [\n",
    "        \n",
    "    ],\n",
    "}\n",
    "\n",
    "# compile the regexs\n",
    "for lang in ['greek', 'latin']:\n",
    "    replacements[lang] = [(re.compile(pat), repl) for pat, repl in replacements[lang]]\n",
    "    \n",
    "\n",
    "# generic tokenize-lemmatize function\n",
    "def lemmatize(text, lang):\n",
    "    '''return a set of (token,lemmata) pairs for a string'''\n",
    "    \n",
    "    for pat, repl in replacements[lang]:\n",
    "        text = pat.sub(repl, text)\n",
    "    \n",
    "    tokens = tokenizer[lang].tokenize(text)\n",
    "    lemmata = lemmatizer[lang].lemmatize(tokens)\n",
    "    \n",
    "    return lemmata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WikiData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwikidata.linked_data_interface import get_entity_dict_from_api\n",
    "from qwikidata.entity import WikidataItem, WikidataProperty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 1\n",
    "\n",
    "Let's start by building a lexicon for all the words Telemachus speaks to Penelope.\n",
    "\n",
    "### Identify and download the speeches\n",
    "\n",
    "Using the hand-rolled DICES API code, we can search speeches using keywords. For now, JSON results from the API are paged, so if your search has a lot of results, you may have to wait for several pages to download. I've added a progress bar widget because I get impatient.\n",
    "\n",
    "Note that I can specify both the speaker and the addressee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = api.getSpeeches(spkr_name='Telemachus', addr_name='Penelope')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in speeches:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the passages from a remote library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = []\n",
    "for s in speeches:\n",
    "    cts_passage = s.getCTS()\n",
    "    text = cts_passage.text\n",
    "    passages.append(text)\n",
    "    \n",
    "    print(f'{s.author.name} {s.work.title} {s.l_range}')\n",
    "    print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CLTK to parse the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lems = Counter()\n",
    "for p in passages:\n",
    "    lang = s.getLang()\n",
    "    lemmatized = lemmatize(p.lower(), lang)\n",
    "    \n",
    "    these_lems = [lem for tok, lem in lemmatized]\n",
    "    lems.update(these_lems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(lems.most_common(), columns=['lemma', 'count'])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Now let's think more broadly. How typical is this kind of speech? We can use external linked data to find other examples of mother-son conversations in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some custom code to query WikiData\n",
    "\n",
    "This lets us ask whether a given addressee belongs to the set of people having a certain relationship to a given speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkWD(c):\n",
    "    '''make sure character has wikidata id'''\n",
    "    if c.char is not None:\n",
    "        if c.char.wd is not None:\n",
    "            if len(c.char.wd.strip()) > 0:\n",
    "                return c.char.wd.strip()\n",
    "\n",
    "def checkWDRelation(s, a, relation, cache={}):\n",
    "    if (s.id, a.id) in cache:\n",
    "        return cache[(s.id, a.id)]\n",
    "\n",
    "    res = False\n",
    "\n",
    "    if not hasattr(s, 'wd_ent'):\n",
    "        s.wd_ent = WikidataItem(get_entity_dict_from_api(s.wd))\n",
    "\n",
    "    claim_group = s.wd_ent.get_truthy_claim_group(relation)\n",
    "    for claim in claim_group:\n",
    "        if claim.mainsnak.datavalue is None:\n",
    "            continue\n",
    "        if claim.mainsnak.datavalue.value['id'] == a.wd:\n",
    "            res = True\n",
    "    \n",
    "    cache[(s.id, a.id)] = res\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the relation \"mother of\" has the WikiData ID `'P25'`. Here's how we ask if a given addressee is the mother of a given speaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker = api.getCharacters(name='Telemachus')[0]\n",
    "addressee = api.getCharacters(name='Penelope')[0]\n",
    "\n",
    "checkWDRelation(speaker, addressee, 'P25')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using WikiData to filter the speeches\n",
    "\n",
    "The DICES dataset includes WikiData ids for most of the characters (not all). The DICES API doesn't let us query WikiData itself, though. For now, the easiest thing for now is just to download all the speeches and character IDs, and then cross reference them against WikiData using its own API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all the speeches: takes a minute\n",
    "speeches = api.getSpeeches(progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_mothers = {}\n",
    "cache_fathers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "\n",
    "# create a progress bar\n",
    "pbar = widgets.IntProgress(\n",
    "    value = 0,\n",
    "    min = 0,\n",
    "    max = len(speeches),\n",
    "    bar_style='info',\n",
    "    orientation='horizontal'\n",
    ")\n",
    "pbar_label = widgets.Label(value = f'{pbar.value}/{len(speeches)}')\n",
    "display(widgets.HBox([pbar, pbar_label]))\n",
    "\n",
    "for s in speeches:\n",
    "    if s.spkr is not None and s.addr is not None:\n",
    "        for spkr in s.spkr:\n",
    "            spkr_wd = checkWD(spkr)\n",
    "            if spkr_wd is not None:\n",
    "\n",
    "                for addr in s.addr:\n",
    "                    addr_wd = checkWD(addr)\n",
    "                    if addr_wd is not None:\n",
    "                        df.append((\n",
    "                            s.id,\n",
    "                            spkr.char.name, spkr_wd, \n",
    "                            addr.char.name, addr_wd,\n",
    "                            checkWDRelation(spkr.char, addr.char, 'P25', cache=cache_mothers),\n",
    "                            checkWDRelation(spkr.char, addr.char, 'P22', cache=cache_fathers),\n",
    "                            ))\n",
    "    pbar.value += 1\n",
    "    pbar_label.value = f'{pbar.value}/{len(speeches)}'\n",
    "\n",
    "df = pd.DataFrame(df, columns=['id', 'spkr', 'sp_wd', 'addr', 'ad_wd', 'mother', 'father'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['father']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mother = [checkWD(s.)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spkr = speeches[0].data['spkr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tel = dicesapi.CharacterInstance(spkr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insts = [dicesapi.CharacterInstance(c, api=speeches[0].api) for c in speeches[0].data['spkr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insts[0].char.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getsizeof(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getsizeof(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tel = speeches[1164].spkr[0].char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen = speeches[1162].spkr[0].char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tel.wd_ent = WikidataItem(get_entity_dict_from_api(tel.wd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tel.wd_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_group = tel.wd_ent.get_truthy_claim_group('P25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for claim in claim_group:\n",
    "    print(claim.mainsnak.datavalue.value['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies = api.getSpeeches(part=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = api.getSpeeches(part=2)\n",
    "\n",
    "replies = []\n",
    "openings = []\n",
    "\n",
    "for reply in q:\n",
    "    s = api.getSpeeches(cluster_id=reply.cluster.id)\n",
    "    if len(s) > 1:\n",
    "        openings.append(s[0])\n",
    "        replies.append(s[1])\n",
    "        print(s[0].part, s[1].part)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in replies:\n",
    "    print(r, r.part, r.cluster.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opening_texts = []\n",
    "for s in openings:\n",
    "    try:\n",
    "        text = s.getCTS().text\n",
    "    except:\n",
    "        print(f'{s} failed')\n",
    "        text = None\n",
    "    opening_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_texts = []\n",
    "for s in replies:\n",
    "    try:\n",
    "        text = s.getCTS().text\n",
    "    except:\n",
    "        print(f'{s} failed')\n",
    "        text = None\n",
    "    reply_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opening_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
